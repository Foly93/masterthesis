#!/bin/bash

### HEADER
# This script is for multiplying and adjusting a template jupyter notebook so that a cluster of .py programms can run consecutively without human intervention.
# the .py programs generated by this script executes a random forest classifier to train on the Morgan fingerprints of overlapped compounds (between CP-Dataset of Bray et al and various Pubchem Assays)
# The Input data for this notebook is stored in 03-GenerateFingerprints and is part of the master thesis of Luis Vollmers
# The output is stored in the _output directory of this subproject 04-PredictFingerprints and is a single summary file that stored relevant metrics from the classification sorted by AID of the PubchemAssays


### DECLARE GLOBAL VARIABLES

# since I am using a german computer the default decimal is "," which is changed here
export LC_ALL=C

# set global variables for the output
OUT_FILE="summary_results.txt"
OUT_DIR="../_output"
SRC_IPYNB="rf_fp_1030.ipynb"
SRC_PYTHON="rf_fp_1030.py"
TMP_FILE="rf_fp_AID.py"

### DEFINE FUNCTIONS

# first function to call; uses random forest notebooks to generate predictions
# for details see rf_fp_1030.ipynb
generate_predictions () 
{
	# use the nbconvert to generate .py from notebook
	jupyter nbconvert --to script ${SRC_IPYNB}

	# rename the notebook and change the iterable (1030) to placeholder (AID)
	sed "s/1030/AID/g" ${SRC_PYTHON} > ${TMP_FILE}
	# remove the plt.show() which is disturbing the automatic process
	sed -i '/show/d' ${TMP_FILE}

	# for loop iterates over all input files in the 2nd step Dir. which are used in the .py files
	for aid in $(ls ../../02-CorrectingAssays/_output/*)
	do 
		# generate the sole AID and the filename for the loop-executable
		# method is called 'Parameter Substitution in bash"
		aid=${aid%.*}
		aid=${aid##*_}
		EXEC_FILE="rf_fp_${aid}.py"

		# change the placeholder to the concurrent AID and rename the resulting executable
		sed "s/AID/$aid/g" ${TMP_FILE} > ${EXEC_FILE}
		# make file executable
		chmod 777 ${EXEC_FILE}
		
		# make the random forest prediction using Fingerprints and measure time 
		printf "Running for $aid..."
		time ./${EXEC_FILE}
		echo "done"

		# executables are disposable and would cramp up the exec directory; to the TRASH!! 
		mv $EXEC_FILE ../_trash/
	done

	# move temporary files to the trash
	mv ${TMP_FILE} ../_trash/
}

# second funtion summarizes the results
# different metrics are saved to the summary file that give information about the prediction quality
summarize_results () 
{
	# print the date to the newly made output file
	date > ${OUT_DIR}/${OUT_FILE}
	# print the header line to the output file
	printf "%-6s\t%6s\t%6s\t%6s\t%6s\t%6s\n" "AID" "AUC-Sc" "BalAcc" "MatCff" "Sensit" "Specif" >> ${OUT_DIR}/${OUT_FILE}
	
	# iterate over all inputs like in the prior function
	for aid in $(ls ../../02-CorrectingAssays/_output/*)
	do
		# see prior function
		aid=${aid%.*}
	        aid=${aid##*_}
		# set input file as stated in the analysis evaluation of the respective *.ipynb file
		INP_FILE="rf_fp_${aid}_analysis.txt"
	
		# grep the metrics from the individiual analysis files
		# see rf_cp_${aid}_analysis.txt for understanding the grep-pattern in detail
		AUC=$(grep 'AUC' ${OUT_DIR}/${INP_FILE} | awk '{print $2}')
		BA=$(grep 'Bal' ${OUT_DIR}/${INP_FILE} | awk '{print $3}')
		MCC=$(grep 'Mat' ${OUT_DIR}/${INP_FILE} | awk '{print $3}')
		SENS=$(grep 'Sensi' ${OUT_DIR}/${INP_FILE} | awk '{print $2}')
		SPEC=$(grep 'Speci' ${OUT_DIR}/${INP_FILE} | awk '{print $2}')
	
		# print the metrics sorted by AID to the output file 
		printf "%-6d\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\n" "$aid" "$AUC" "$BA" "$MCC" "$SENS" "$SPEC" >> ${OUT_DIR}/${OUT_FILE}
	done
	sed -i 's/ //g' ${OUT_DIR}/${OUT_FILE}
}


### CALL FUNCTIONS
generate_predictions
summarize_results
